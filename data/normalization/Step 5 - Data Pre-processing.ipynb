{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import dill\n",
    "import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pip should not execute arbitrary code from the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Make load safe_load - [ ] Make `yaml.load` def...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CSRF vulnerability CVE-2013-7259 Hi, I'm looki...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Add salt to Array#hash For #2437, we partially...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cross Site Scripting Vulnerability Versions up...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  label\n",
       "0  pip should not execute arbitrary code from the...      2\n",
       "1  Make load safe_load - [ ] Make `yaml.load` def...      2\n",
       "2  CSRF vulnerability CVE-2013-7259 Hi, I'm looki...      2\n",
       "3  Add salt to Array#hash For #2437, we partially...      2\n",
       "4  Cross Site Scripting Vulnerability Versions up...      2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('GH_processed_labeled_issues_prs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((481390,), (120348,), (481390,), (120348,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.description.values\n",
    "y = df.label.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Counter({0: 413990, 1: 66929, 2: 471})\n",
      "Test: Counter({0: 103497, 1: 16744, 2: 107})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print('Train:', Counter(y_train))\n",
    "print('Test:', Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text, re.I)\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = '((https?:\\/\\/)(\\s)*(www\\.)?|(www\\.))(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*'\n",
    "    text = re.sub(url_pattern, ' ', text, re.I)\n",
    "    return text\n",
    "\n",
    "def remove_checklists(text):\n",
    "    checklist_pattern = r'\\[[xX\\.\\s]\\]'\n",
    "    text = re.sub(checklist_pattern, ' ', text, re.I|re.DOTALL)\n",
    "    return text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def pre_process_document(document):\n",
    "    \n",
    "    # strip HTML\n",
    "    document = strip_html_tags(document)\n",
    "    \n",
    "    # remove URLS\n",
    "    document = remove_urls(document)\n",
    "    \n",
    "    # remove checklists\n",
    "    document = remove_checklists(document)\n",
    "    \n",
    "    # expand contractions    \n",
    "    document = expand_contractions(document)\n",
    "    \n",
    "    # lower case\n",
    "    document = document.lower()\n",
    "    \n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    \n",
    "    # remove accented characters\n",
    "    document = remove_accented_chars(document)\n",
    "    \n",
    "    \n",
    "               \n",
    "    # remove special characters and\\or digits    \n",
    "    # insert spaces between special characters to isolate them    \n",
    "    special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "    document = special_char_pattern.sub(\" \\\\1 \", document)\n",
    "    document = remove_special_characters(document, remove_digits=False)  \n",
    "    \n",
    "    # remove only numbers\n",
    "    document = re.sub(r'\\b\\d+\\b', ' ', document)\n",
    "        \n",
    "    # remove extra whitespace\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    document = document.strip()\n",
    "    \n",
    "    return document\n",
    "\n",
    "\n",
    "pre_process_corpus = np.vectorize(pre_process_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18229"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50033it [02:10, 234.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored 50000 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100004it [04:22, 219.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored 50000 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150048it [06:39, 259.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored 50000 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200038it [08:48, 260.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored 50000 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250069it [10:55, 284.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored 50000 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300028it [13:09, 237.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored 50000 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "350040it [15:23, 228.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored 50000 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400035it [17:34, 236.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored 50000 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "450047it [19:44, 276.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored 50000 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481390it [21:04, 380.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored 31390 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for idx, record in tqdm.tqdm(enumerate(X_train)):\n",
    "    doc = pre_process_document(record)\n",
    "    docs.append(doc)\n",
    "    if ((idx+1) % 50000 == 0) or ((idx+1) == len(X_train)):\n",
    "        with open('Xtrain_norm.pkl', 'ab+') as f:\n",
    "            dill.dump(docs, f)\n",
    "        print('Processed and stored {} docs'.format(len(docs)))\n",
    "        docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15514"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50029it [02:04, 234.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored 50000 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100025it [04:17, 210.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored 50000 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120348it [05:10, 387.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored 20348 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for idx, record in tqdm.tqdm(enumerate(X_test)):\n",
    "    doc = pre_process_document(record)\n",
    "    docs.append(doc)\n",
    "    if ((idx+1) % 50000 == 0) or ((idx+1) == len(X_test)):\n",
    "        with open('Xtest_norm.pkl', 'ab+') as f:\n",
    "            dill.dump(docs, f)\n",
    "        print('Processed and stored {} docs'.format(len(docs)))\n",
    "        docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11190"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ytrain_labels.pkl', 'wb') as f:\n",
    "    dill.dump(y_train, f)\n",
    "with open('ytest_labels.pkl', 'wb') as f:\n",
    "    dill.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
