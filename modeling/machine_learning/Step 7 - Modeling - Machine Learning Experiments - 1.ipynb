{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import dill\n",
    "import tqdm\n",
    "import gc\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-05 05:58:03,252 : INFO : loading Word2Vec object from ./basic_w2v_sg10ep.bin\n",
      "2019-02-05 05:58:03,549 : INFO : loading wv recursively from ./basic_w2v_sg10ep.bin.wv.* with mmap=None\n",
      "2019-02-05 05:58:03,550 : INFO : loading vectors from ./basic_w2v_sg10ep.bin.wv.vectors.npy with mmap=None\n",
      "2019-02-05 05:58:03,620 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-02-05 05:58:03,621 : INFO : loading vocabulary recursively from ./basic_w2v_sg10ep.bin.vocabulary.* with mmap=None\n",
      "2019-02-05 05:58:03,622 : INFO : loading trainables recursively from ./basic_w2v_sg10ep.bin.trainables.* with mmap=None\n",
      "2019-02-05 05:58:03,622 : INFO : loading syn1neg from ./basic_w2v_sg10ep.bin.trainables.syn1neg.npy with mmap=None\n",
      "2019-02-05 05:58:03,694 : INFO : setting ignored attribute cum_table to None\n",
      "2019-02-05 05:58:03,694 : INFO : loaded ./basic_w2v_sg10ep.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=148741, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec.load('./basic_w2v_sg10ep.bin')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOF reached\n",
      "EOF reached\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(481390, 120348, 481390, 120348)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Xtrain_norm.pkl', 'rb') as f:\n",
    "    X_train = []\n",
    "    while True:\n",
    "        try:\n",
    "            X_train.extend(dill.load(f))\n",
    "        except:\n",
    "            print('EOF reached')\n",
    "            break\n",
    "            \n",
    "with open('Xtest_norm.pkl', 'rb') as f:\n",
    "    X_test = []\n",
    "    while True:\n",
    "        try:\n",
    "            X_test.extend(dill.load(f))\n",
    "        except:\n",
    "            print('EOF reached')\n",
    "            break\n",
    "            \n",
    "with open('ytrain_labels.pkl', 'rb') as f:\n",
    "    y_train = dill.load(f)\n",
    "    \n",
    "with open('ytest_labels.pkl', 'rb') as f:\n",
    "    y_test = dill.load(f)\n",
    "    \n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# tokenize sentences in corpus\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_train = [wpt.tokenize(document) for document in X_train]\n",
    "tokenized_test = [wpt.tokenize(document) for document in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    " \n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((481390, 300), (120348, 300))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get document level embeddings\n",
    "train_features = averaged_word_vectorizer(corpus=tokenized_train, model=model,\n",
    "                                             num_features=model.vector_size)\n",
    "\n",
    "test_features = averaged_word_vectorizer(corpus=tokenized_test, model=model,\n",
    "                                             num_features=model.vector_size)\n",
    "\n",
    "train_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Xtrain_embeddings_w2v.pkl', 'wb') as f:\n",
    "    dill.dump(train_features, f)\n",
    "    \n",
    "with open('./Xtest_embeddings_w2v.pkl', 'wb') as f:\n",
    "    dill.dump(test_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67400, 300), (67400,), (16851, 300), (16851,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_positives = []\n",
    "y_train_positives = []\n",
    "for idx, label in enumerate(y_train):\n",
    "    if label != 0:\n",
    "        train_positives.append(train_features[idx])\n",
    "        y_train_positives.append(label)\n",
    "\n",
    "test_positives = []\n",
    "y_test_positives = []\n",
    "for idx, label in enumerate(y_test):\n",
    "    if label != 0:\n",
    "        test_positives.append(test_features[idx])\n",
    "        y_test_positives.append(label)\n",
    "        \n",
    "train_positives = np.array(train_positives)\n",
    "y_train_positives = np.array(y_train_positives)\n",
    "test_positives = np.array(test_positives)\n",
    "y_test_positives = np.array(y_test_positives)\n",
    "train_positives.shape, y_train_positives.shape, test_positives.shape, y_test_positives.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0816            1.66m\n",
      "         2           0.0785            1.67m\n",
      "         3           0.0775            1.67m\n",
      "         4           0.0757            1.65m\n",
      "         5           0.0748            1.62m\n",
      "         6           0.0735            1.59m\n",
      "         7           0.0727            1.56m\n",
      "         8           0.0717            1.53m\n",
      "         9           0.0710            1.50m\n",
      "        10           0.0705            1.47m\n",
      "        11           0.0698            1.44m\n",
      "        12           0.0688            1.42m\n",
      "        13           0.0686            1.39m\n",
      "        14           0.0680            1.36m\n",
      "        15           0.0675            1.32m\n",
      "        16           0.0671            1.28m\n",
      "        17           0.0667            1.24m\n",
      "        18           0.0660            1.20m\n",
      "        19           0.0655            1.17m\n",
      "        20           0.0650            1.13m\n",
      "        21           0.0646            1.09m\n",
      "        22           0.0642            1.05m\n",
      "        23           0.0638            1.01m\n",
      "        24           0.0634           58.53s\n",
      "        25           0.0632           56.22s\n",
      "        26           0.0626           53.92s\n",
      "        27           0.0622           51.66s\n",
      "        28           0.0617           49.38s\n",
      "        29           0.0615           47.04s\n",
      "        30           0.0611           44.79s\n",
      "        31           0.0608           42.46s\n",
      "        32           0.0602           40.25s\n",
      "        33           0.0598           38.00s\n",
      "        34           0.0595           35.90s\n",
      "        35           0.0592           33.66s\n",
      "        36           0.0588           31.39s\n",
      "        37           0.0585           29.14s\n",
      "        38           0.0582           26.86s\n",
      "        39           0.0579           24.64s\n",
      "        40           0.0576           22.40s\n",
      "        41           0.0573           20.23s\n",
      "        42           0.0570           18.03s\n",
      "        43           0.0566           15.76s\n",
      "        44           0.0562           13.53s\n",
      "        45           0.0558           11.28s\n",
      "        46           0.0555            9.03s\n",
      "        47           0.0552            6.77s\n",
      "        48           0.0549            4.51s\n",
      "        49           0.0546            2.26s\n",
      "        50           0.0543            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=2,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "lr = GradientBoostingClassifier(n_estimators=50, verbose=2)\n",
    "lr.fit(train_positives, y_train_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.993\n",
      "Precision: 0.9886\n",
      "Recall: 0.993\n",
      "F1 Score: 0.9905\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.99      1.00      1.00     16744\n",
      "          2       0.18      0.03      0.05       107\n",
      "\n",
      "avg / total       0.99      0.99      0.99     16851\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "          Predicted:    \n",
      "                   1   2\n",
      "Actual: 1      16730  14\n",
      "        2        104   3\n"
     ]
    }
   ],
   "source": [
    "import model_evaluation_utils as meu\n",
    "\n",
    "y_pred = lr.predict(test_positives)\n",
    "meu.display_model_performance_metrics(true_labels=y_test_positives, predicted_labels=y_pred, \n",
    "                                      classes=list(set(y_test_positives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma='auto', kernel='rbf',\n",
       "      max_iter=-1, nu=0.5, random_state=None, shrinking=True, tol=0.001,\n",
       "      verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "osvm = OneClassSVM()\n",
    "osvm.fit(train_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = osvm.predict(test_positives)\n",
    "preds = [1 if item == -1 else 2 for item in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 10566, 2: 6285})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.6273\n",
      "Precision: 0.9889\n",
      "Recall: 0.6273\n",
      "F1 Score: 0.7653\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      0.63      0.77     16744\n",
      "          2       0.01      0.52      0.02       107\n",
      "\n",
      "avg / total       0.99      0.63      0.77     16851\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "          Predicted:      \n",
      "                   1     2\n",
      "Actual: 1      10515  6229\n",
      "        2         51    56\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=y_test_positives, predicted_labels=preds, \n",
    "                                      classes=list(set(y_test_positives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=4, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1, verbose=2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgc = xgb.XGBClassifier(max_depth=5, n_jobs=4, verbose=1)\n",
    "xgc.fit(train_positives, y_train_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.993\n",
      "Precision: 0.9886\n",
      "Recall: 0.993\n",
      "F1 Score: 0.9905\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.99      1.00      1.00     16744\n",
      "          2       0.18      0.03      0.05       107\n",
      "\n",
      "avg / total       0.99      0.99      0.99     16851\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "          Predicted:    \n",
      "                   1   2\n",
      "Actual: 1      16730  14\n",
      "        2        104   3\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(test_positives)\n",
    "meu.display_model_performance_metrics(true_labels=y_test_positives, predicted_labels=y_pred, \n",
    "                                      classes=list(set(y_test_positives)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
