{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Step 8b - Modeling - Deep Learning - CVE vs Non-CVE - v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "1veJ0YPWrPgd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CVE vs. Non-CVE Prediction - Deep Learning with Bi-directional GRUs "
      ]
    },
    {
      "metadata": {
        "id": "wXCdw2f3X9Vo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f9442da6-ae94-4365-ce8f-5572e89bfeba"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-iPklD4UYIXF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a0f9d97e-1881-40a3-bc4c-45bcf7da0662"
      },
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Colab Notebooks'   dl_model.h5   Xtest_norm.pkl    ytest_labels.pkl\n",
            " dl_model2.h5\t    file.txt\t  Xtrain_norm.pkl   ytrain_labels.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LheJl7h3fEIM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a51d87b-8b97-4e43-fdf4-891bed23b1dc"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import dill\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, CuDNNLSTM\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5l1rpVXjrK_n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Retrieval"
      ]
    },
    {
      "metadata": {
        "id": "sX6jamWBp3qu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Following is normalized (pre-processed text data) issue\\PR descriptions and corresponding labels which I had pickled earlier.\n",
        "- This is the full data which we have BTW including positives and negatives \n",
        "- class label 0 - non-security related data potentially which was unflagged by Regexes\n",
        "- class label 1 - potentially security related data which was flagged by Regexes\n",
        "- class label 2 - security and CVE related data which we manually mapped"
      ]
    },
    {
      "metadata": {
        "id": "VOuHAKNLfM9p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cc6d36b0-2842-448e-d99b-3f48ebc7e811"
      },
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/Xtrain_norm.pkl', 'rb') as f:\n",
        "    X_train = []\n",
        "    while True:\n",
        "        try:\n",
        "            X_train.extend(dill.load(f))\n",
        "        except:\n",
        "            print('EOF reached')\n",
        "            break\n",
        "            \n",
        "with open('/content/drive/My Drive/Xtest_norm.pkl', 'rb') as f:\n",
        "    X_test = []\n",
        "    while True:\n",
        "        try:\n",
        "            X_test.extend(dill.load(f))\n",
        "        except:\n",
        "            print('EOF reached')\n",
        "            break\n",
        "            \n",
        "with open('/content/drive/My Drive/ytrain_labels.pkl', 'rb') as f:\n",
        "    y_train = dill.load(f)\n",
        "    \n",
        "with open('/content/drive/My Drive/ytest_labels.pkl', 'rb') as f:\n",
        "    y_test = dill.load(f)\n",
        "    \n",
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EOF reached\n",
            "EOF reached\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(481390, 120348, 481390, 120348)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "dtIHYCixqh67",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- We filter out the negative data (non-security related) in the following code to reduce the class imbalance.\n",
        "- We focus only on modeling for security data i.e CVEs vs. Non-CVEs"
      ]
    },
    {
      "metadata": {
        "id": "4zJKPV_ofj4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c2be705-55e7-459e-b5ae-e01fe947128e"
      },
      "cell_type": "code",
      "source": [
        "train_positives = []\n",
        "y_train_positives = []\n",
        "for doc, label in zip(X_train, y_train):\n",
        "    if label != 0:\n",
        "        train_positives.append(doc)\n",
        "        y_train_positives.append(label)\n",
        "\n",
        "test_positives = []\n",
        "y_test_positives = []\n",
        "for doc, label in zip(X_test, y_test):\n",
        "    if label != 0:\n",
        "        test_positives.append(doc)\n",
        "        y_test_positives.append(label)\n",
        "        \n",
        "len(train_positives), len(y_train_positives), len(test_positives), len(y_test_positives)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67400, 67400, 16851, 16851)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "z0rrGSmXrIHg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "metadata": {
        "id": "QZieJ1CEfpeR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5523121f-a81c-4617-888d-20811e321451"
      },
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(train_positives, y_train_positives, test_size=0.1, random_state=42)\n",
        "X_test, y_test = test_positives, y_test_positives\n",
        "len(X_train), len(X_val), len(X_test), len(y_train), len(y_val), len(y_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60660, 6740, 16851, 60660, 6740, 16851)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "uQGDWo3rfsd5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## some config values \n",
        "embed_size = 300 # how big is each word vector\n",
        "max_features = 300000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "maxlen = 1000 # max number of words in a doc to use"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kRdExOfcfvWx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Tokenize the sentences\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "train_X = tokenizer.texts_to_sequences(X_train)\n",
        "val_X = tokenizer.texts_to_sequences(X_val)\n",
        "test_X = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IB7g9_ZyfxvR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Pad the sentences \n",
        "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
        "test_X = pad_sequences(test_X, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OatbDgGOgJg5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_y = np.array([1 if item==2 else 0 for item in y_train])\n",
        "val_y = np.array([1 if item==2 else 0 for item in y_val])\n",
        "test_y = np.array([1 if item==2 else 0 for item in y_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WRaUIxUNlfdT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "16c9d541-72ee-4f79-f666-274b56b7dff5"
      },
      "cell_type": "code",
      "source": [
        "print('Sample Data:')\n",
        "print(X_train[:3])\n",
        "print(train_X[:3, :])\n",
        "print(train_y[:3])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Data:\n",
            "['fix regression introduced during previous scanner hardening module workdir is deleted when cleaning root module work dir when they are nested', 'utf passwords in url do not work summary curl s url enganacao data method help works usr bin env python2 import requests import json payload method help url enganacao requests post url headers content type text plain charset utf data json dumps payload also works but use usr bin env python3 instead and it does not work anymore requests messes things up expected result work as curl and requests in python2 do actual result requests in python3 messes with encoding reproduction steps python3 import requests import json payload method help url http user1 enganacao requests post url headers content type text plain charset utf data json dumps payload system information python', 'add module to dump gnome keyring network passwords this pr adds a new post module that will dump network passwords from gnome keyring on linux systems this leverages the newly submitted railgun for linux support to make the applicable api calls directly to libgnome keyring so as the current user the current user does not have to be root they just need to be using gnome keyring the module is a bit slow due to the necessary railgun memread calls that access the different strings returned in the gnomekeyringnetworkpassworddata struct hostnames are looked up using meterpreter so they are resolved from the perspective of where the information came from this is required to place the credentials into the database with a corresponding host object it requires the following two prs to be accepted first but demonstrates the real world value in them rapid7 metasploit payloads rapid7 metasploit framework verification list the steps needed to make sure this thing works start msfconsole get a python meterpreter session on a linux host using gnome and gnome keyring run the post linux gather gnome keyring dump see plain text passwords right before your eyes applicable entries have been added to the database example output unfortunately the entire output of this module when verbose is false would basically be censored tested on fedora x86']\n",
            "[[   0    0    0 ...  562   73  610]\n",
            " [   0    0    0 ...   98  399   58]\n",
            " [   0    0    0 ...   27 2830  138]]\n",
            "[0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eaHAQXw-qtpE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We build a `class_weight` dictionary to tell our deep learning model to give higher weightage to each CVE related issue. Based on the weights computed below, you can see the class imbalance is terrible still, we definitely need more +ve CVE related data (each record helps)"
      ]
    },
    {
      "metadata": {
        "id": "X5MAsjkKhh5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2270c18-4f6a-4232-a882-e34f8e85f46e"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(train_y),\n",
        "                                                 train_y)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "class_weights[0] /= 2 #0.1  #0.05 | 1\n",
        "class_weights[1] *= 4#300   #650 | 100\n",
        "class_weights"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.25183081751606634, 1: 275.1020408163265}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "o1AtZ5iwv9Ny",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XB-wVOxarB5i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ]
    },
    {
      "metadata": {
        "id": "On8aPw5if1Rj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "5c3641aa-4efa-4d44-8e00-a8cc1351ca85"
      },
      "cell_type": "code",
      "source": [
        "inp = Input(shape=(maxlen,))\n",
        "x = Embedding(max_features, embed_size)(inp)\n",
        "x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
        "x = GlobalMaxPool1D()(x)\n",
        "x = Dense(16, activation=\"relu\")(x)\n",
        "x = Dropout(rate=0.1)(x)\n",
        "x = Dense(1, activation=\"sigmoid\")(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 1000, 300)         90000000  \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 1000, 128)         140544    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                2064      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 90,142,625\n",
            "Trainable params: 90,142,625\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M6g2Qhv_h9og",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "0dfeb61f-7772-4742-e6c0-6559f60f0685"
      },
      "cell_type": "code",
      "source": [
        "model.fit(train_X, train_y, batch_size=512, epochs=20, initial_epoch=10,\n",
        "          class_weight=class_weights, validation_data=(val_X, val_y))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60660 samples, validate on 6740 samples\n",
            "Epoch 11/20\n",
            "60660/60660 [==============================] - 99s 2ms/step - loss: 0.0084 - acc: 0.9943 - val_loss: 0.0999 - val_acc: 0.9736\n",
            "Epoch 12/20\n",
            "60660/60660 [==============================] - 99s 2ms/step - loss: 0.0666 - acc: 0.9517 - val_loss: 0.1234 - val_acc: 0.9642\n",
            "Epoch 13/20\n",
            "60660/60660 [==============================] - 99s 2ms/step - loss: 0.0167 - acc: 0.9863 - val_loss: 0.1380 - val_acc: 0.9642\n",
            "Epoch 14/20\n",
            "60660/60660 [==============================] - 99s 2ms/step - loss: 0.0322 - acc: 0.9804 - val_loss: 0.6752 - val_acc: 0.8043\n",
            "Epoch 15/20\n",
            "60660/60660 [==============================] - 98s 2ms/step - loss: 0.0283 - acc: 0.9750 - val_loss: 0.1012 - val_acc: 0.9700\n",
            "Epoch 16/20\n",
            "60660/60660 [==============================] - 99s 2ms/step - loss: 0.0125 - acc: 0.9908 - val_loss: 0.0893 - val_acc: 0.9764\n",
            "Epoch 17/20\n",
            "60660/60660 [==============================] - 98s 2ms/step - loss: 0.0077 - acc: 0.9945 - val_loss: 0.0837 - val_acc: 0.9794\n",
            "Epoch 18/20\n",
            "60660/60660 [==============================] - 98s 2ms/step - loss: 0.0061 - acc: 0.9955 - val_loss: 0.0792 - val_acc: 0.9813\n",
            "Epoch 19/20\n",
            "60660/60660 [==============================] - 98s 2ms/step - loss: 0.0061 - acc: 0.9958 - val_loss: 0.1171 - val_acc: 0.9721\n",
            "Epoch 20/20\n",
            "60660/60660 [==============================] - 98s 2ms/step - loss: 0.0061 - acc: 0.9953 - val_loss: 0.0707 - val_acc: 0.9835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f086250ed68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "tvmTMuWxrEXC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "xZRe-q4DiVpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1024ba17-6f3b-48c2-88ee-6279a6a54515"
      },
      "cell_type": "code",
      "source": [
        "pred_y = model.predict([test_X], batch_size=512, verbose=1)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16851/16851 [==============================] - 8s 488us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "csuNSO8vkXyd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_y = pred_y.ravel()\n",
        "pred_y = [1 if prob > 0.5 else 0 for prob in pred_y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0DY8pgGVkaC1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l7ldIjshkpdS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "62ea0c87-af7e-4adb-c6a0-0073f538de1b"
      },
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_true=test_y, y_pred=pred_y)\n",
        "# we can get more recall at the cost of more false positives\n",
        "#array([[15377,  1367],\n",
        "#       [   50,    57]])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16501,   243],\n",
              "       [   77,    30]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "X9Oxibjek7kf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "42191a76-814a-46b7-e80c-c90819860ef5"
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true=test_y, y_pred=pred_y))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99     16744\n",
            "           1       0.11      0.28      0.16       107\n",
            "\n",
            "   micro avg       0.98      0.98      0.98     16851\n",
            "   macro avg       0.55      0.63      0.57     16851\n",
            "weighted avg       0.99      0.98      0.99     16851\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZD-Rep9pu_-G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/My Drive/dl_model4.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sbKhu7d4lGsV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- This model uses a Bidirectional GRU + trying to tackle class imbalance with class weights\n",
        "- Embeddings are trained from scratch\n",
        "- We have trained only on Security related data (filtering out the -ves completely with regex) CVE and maybe non-CVE related\n",
        "- 30 out of 107 we are able to predict correctly on the test set (28% recall)\n",
        "- We need more positive data definitely\n",
        "- We can even adjust class weights to predict more CVE data but false positives also increase\n"
      ]
    },
    {
      "metadata": {
        "id": "H7CuEO-EERIq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}